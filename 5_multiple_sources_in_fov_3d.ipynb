{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0fc0e82-639a-4fc4-9e0a-173d63a924fe",
   "metadata": {},
   "source": [
    "# 5. Analysis of two sources in the same field of view - 3D analysis\n",
    "Author: Marcel Strzys (strzys@icrr.u-tokyo.ac.jp)\n",
    "\n",
    "## 5.1. Context\n",
    "We will repeat the same analysis as in the previous notebook, but using a three-dimensional analysis. This type of analysis uses the spatial information of the events and is particularly useful if we want to model different sources in the same field of view at once (not performing separate data reductions, as we did in the previous notebook). As we take into account the spatial distribution of the events, we can use this analysis to study the morphology of the sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6477c107-adb0-48cf-b781-31fc3a72e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - basic imports (numpy, astropy, regions, matplotlib)\n",
    "import logging\n",
    "import warnings\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "import numpy as np\n",
    "from regions import PointSkyRegion, CircleSkyRegion\n",
    "from scipy.stats import norm\n",
    "\n",
    "# - Gammapy's imports\n",
    "from gammapy.data import DataStore\n",
    "from gammapy.datasets import Datasets\n",
    "from gammapy.datasets import MapDataset\n",
    "\n",
    "from gammapy.estimators import ExcessMapEstimator\n",
    "from gammapy.estimators import FluxPointsEstimator, FluxPoints\n",
    "from gammapy.estimators import TSMapEstimator\n",
    "\n",
    "from gammapy.makers import FoVBackgroundMaker\n",
    "from gammapy.makers import MapDatasetMaker\n",
    "from gammapy.makers import SafeMaskMaker\n",
    "\n",
    "from gammapy.irf import FoVAlignment, Background3D\n",
    "from gammapy.maps import MapAxis\n",
    "from gammapy.maps import WcsGeom\n",
    "\n",
    "from gammapy.modeling import Fit\n",
    "from gammapy.modeling.models import (\n",
    "    PowerLawSpectralModel,\n",
    "    FoVBackgroundModel,\n",
    "    Models,\n",
    "    PiecewiseNormSpectralModel,\n",
    "    PointSpatialModel,\n",
    "    SkyModel,\n",
    ")\n",
    "\n",
    "from plot_utils import plot_gammapy_sed\n",
    "\n",
    "# - setting up logging and ignoring warnings\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a49051-3043-44cf-91dc-cf7e696d4b10",
   "metadata": {},
   "source": [
    "## 4.2. DL3 files for the 3D analysis\n",
    "\n",
    "Let us proceed now with the data reduction. As we said, we are interested in preserving the spatial information of the events, which we loose in a one-dimensional analysis (once, we create our _on_ and _off_ regions, we do not care anymore about the position of the events inside these regions, and we just consider their distribution as a function of the energy). In a 3D analysis we create a three-dimensional histograms of the events spatial coordinates and energy, called a data _cube_. The spatial coordinates are represented, in the `FITS` standard by the so-called World Coordinate System (WCS), which we will also use to define the geometry of our data cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b71aa6f-3f63-45e5-8376-4de0ee7eff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data store\n",
    "data_store = DataStore.from_dir(\n",
    "    \"../acme_magic_odas_data/data/1ES1218+304/full_enclosure\"\n",
    ")\n",
    "observations = data_store.get_observations(required_irf=[\"aeff\", \"edisp\", \"psf\", \"bkg\"])\n",
    "\n",
    "# quick bugfix for Background3D IRF alignment issue\n",
    "# see https://github.com/gammapy/gammapy/issues/3510\n",
    "# and https://github.com/gammapy/gammapy/pull/4667\n",
    "for obs in observations:\n",
    "    new_bkg = Background3D(\n",
    "        axes=obs.bkg.axes,\n",
    "        data=obs.bkg.data,\n",
    "        unit=obs.bkg.unit,\n",
    "        meta=obs.bkg.meta,\n",
    "        fov_alignment=FoVAlignment.REVERSE_LON_RADEC,\n",
    "    )\n",
    "    # Assign the new background IRF to the observation\n",
    "    obs.bkg = new_bkg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ada25-0a05-4971-b0d3-7302ae54033c",
   "metadata": {},
   "source": [
    "Let us take a look at the observations we loaded, especially at the IRF provided for this type of analysis (the argument of the `required_irf` parameter of the `DataStore.get_observations` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112b504-9fba-4ce9-a108-0406e053a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations[0].peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c6ffc",
   "metadata": {},
   "source": [
    "As for the previous notebook, and analysis, we see that the IRF are now multi-offset. But we notice that there are two new IRF components: the Point Spread Function (PSF) and the background. The PSF is the distribution function of the position estimator, and is needed to model the spatial distribution of the events. The background is instead needed to model the spatial distribution of the background events. Remember now we want to model the emission in each point of the field of view, thus we need to know how the background is distributed in the entire field of view, adn we cannot simply cut out a region to estimate it, as we did before. The background was pre-computed and attached to each of the DL3 files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c19fcc-c5c1-4570-b823-508dd5461ef6",
   "metadata": {},
   "source": [
    "## 4.3. Data Reduction\n",
    "\n",
    "We now create our data cube and fill it with the events of our observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e447bb-4a8a-4515-91c7-47b22f789620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source coordinates\n",
    "_1ES1215_coordinates = SkyCoord.from_name(\"1ES1215+303\", frame=\"icrs\")\n",
    "_1ES1218_coordinates = SkyCoord.from_name(\"1ES1218+304\", frame=\"icrs\")\n",
    "\n",
    "# energy axes\n",
    "energy_min = 10 * u.GeV\n",
    "energy_max = 1e5 * u.GeV\n",
    "n_energy_est_bins = 20\n",
    "n_energy_true_bins = 28\n",
    "\n",
    "energy_axis = MapAxis.from_energy_bounds(\n",
    "    energy_min,\n",
    "    energy_max,\n",
    "    n_energy_est_bins,\n",
    "    per_decade=False,\n",
    "    unit=\"GeV\",\n",
    "    name=\"energy\",\n",
    ")\n",
    "energy_true_axis = MapAxis.from_energy_bounds(\n",
    "    energy_min,\n",
    "    energy_max,\n",
    "    n_energy_true_bins,\n",
    "    per_decade=False,\n",
    "    unit=\"GeV\",\n",
    "    name=\"energy_true\",\n",
    ")\n",
    "\n",
    "# spatial binning\n",
    "width = (2.0, 2.0)\n",
    "binsz = 0.02\n",
    "npix = (int(width[0] / binsz), int(width[1] / binsz))\n",
    "\n",
    "geom = WcsGeom.create(\n",
    "    skydir=_1ES1215_coordinates,\n",
    "    npix=npix,\n",
    "    binsz=binsz,\n",
    "    frame=\"icrs\",\n",
    "    proj=\"AIR\",\n",
    "    axes=[energy_axis],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc7be0c-ac7b-4d7f-932b-c1c914c5264b",
   "metadata": {},
   "source": [
    "As we aim to create a data cube, we will now use a `MapDataset`, instead of a `SpectrumDataset` as before.\n",
    "The `MapDatasetMaker` will fill the counts cube and will also convert the irf components from camera coordinates into skycoordinates.\n",
    "\n",
    "We define in addition a safe mask, which exludes all the regions beyond $2^{\\rm circ}$ from the centre of our telescopes, as the irfs in that region are insufficiently sampled and thus not well determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9650ac0-13d9-4fcb-a22c-1b79d6a74524",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_map_dataset = MapDataset.create(geom=geom)\n",
    "datasets = Datasets()\n",
    "maker = MapDatasetMaker(selection=[\"counts\", \"background\", \"psf\", \"edisp\", \"exposure\"])\n",
    "maker_safe_mask = SafeMaskMaker(methods=[\"offset-max\"], offset_max=2.0 * u.deg)\n",
    "\n",
    "for obs in observations:\n",
    "    dataset = maker.run(empty_map_dataset.copy(), obs)\n",
    "    dataset = maker_safe_mask.run(dataset, obs)\n",
    "    datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580eb2f-b9d0-49bc-a8c1-d0692bbf7a90",
   "metadata": {},
   "source": [
    "Let us take a look at the datasets we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa71c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[0].peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c2d4bb-d566-40d5-a456-17530a329165",
   "metadata": {},
   "source": [
    "We can see the counts, background, excesses (counts - background), and exposure as a function of sky coordinates (all the energies are shown).\n",
    "\n",
    "### 4.3.1. Adjusting the background\n",
    "The background we embedded in the data should already be a reasonable estimation, but we can improve it by further adjusting the background model to each data set.\n",
    "This is done by the `FoVBackgroundMaker`, that fits the background model to the data, excluding regions around known sources.\n",
    "\n",
    "For this we want to exclude emission from sources from the normalisation of the background as it may bias the estimate. In both cases we choose a region of 0.2 deg, matching roughly the containment radius of a point source. This initial adjustment has not influence later when fitting the background again with the source to extract the spectrum, but is meant to result in better skymaps for the initial check for a source detection.\n",
    "\n",
    "For further details on background estimation you can checkout these references: <br>\n",
    "[Malyshev, Mohrmann (2023)](https://ui.adsabs.harvard.edu/abs/2023hxga.book..137M/abstract)   \n",
    "[de Naruois (2021)](https://ui.adsabs.harvard.edu/abs/2021Univ....7..421D/abstract)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45750ff-7816-40b3-abb0-159e1e380c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_radius = 0.3 * u.deg\n",
    "\n",
    "_1ES1215_circle = CircleSkyRegion(center=_1ES1215_coordinates, radius=excl_radius)\n",
    "_1ES1218_circle = CircleSkyRegion(center=_1ES1218_coordinates, radius=excl_radius)\n",
    "exclusion_regions = [_1ES1215_circle, _1ES1218_circle]\n",
    "\n",
    "geom_image = geom.to_image()\n",
    "exclusion_mask = ~geom_image.region_mask(exclusion_regions)\n",
    "\n",
    "fov_background_maker = FoVBackgroundMaker(method=\"fit\", exclusion_mask=exclusion_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e056a68-ca07-4ef3-b021-ff4eef9f1efa",
   "metadata": {},
   "source": [
    "Now we loop over the datasets and adjust the background for each of them. For each we check the factor by which the background needs to be adjusted. If the factor is rather large/small, it suggests that the background model is not a good description of the data (either just the rate is very different or - more concerning - the shape does not match the data well). In this case we better exclude those runs. It should in general not apply to more than 10% of the runs or one should revise the method used for the background model creation (not part of this tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edddb0de-3c1e-4dd8-9495-a581cbabb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_fitted_background = Datasets()\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset = fov_background_maker.run(dataset)\n",
    "    fit_result = dataset.models.to_parameters_table()\n",
    "    norm_fit = fit_result[fit_result[\"name\"] == \"norm\"][\"value\"][0]\n",
    "    if (norm_fit > 1.5) or (norm_fit < 0.5):\n",
    "        logger.warning(\n",
    "            f\"Fit for dataset {dataset.name} exceeds recommended limits. The norm of the fit is {norm_fit}.\"\n",
    "        )\n",
    "        continue\n",
    "    datasets_fitted_background.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a276b17c-5519-4335-afe0-c1ea8f88af3a",
   "metadata": {},
   "source": [
    "As we already seen, there are two different analysis approaches: a stacked and a joint analysis. For the stacked we combine the observations into a single dataset. For the joint we keep the datasets sepearately and fit the models to the separate datasets via a joint likelihood fit. The stacked method is faster, whereas the joint fit should be more precise as it better considers subtle differences between the runs. The stacked method might be helpful for a 3D analysis case, to help reduce computation time (now we have to predict counts not only in energy bins, but also in spatial bins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e2e39-5d36-4a6c-a006-7d6ff57ef97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stacked = datasets_fitted_background.stack_reduce()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeb75ff-19a1-45c2-a90e-8d3db75355c6",
   "metadata": {},
   "source": [
    "Now we can plot the counts, background, and excess (counts -  background). It gives you a first impression how well the maps are populated and whether a potential source is visible in the maps.\n",
    "\n",
    "The maps plotted by gammapy are interactive, so you can select the scaling for the colorscale and the energy bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc2906-f9b6-4dcf-87bd-dd788fd83207",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_radius = 0.1 * u.deg\n",
    "\n",
    "dataset_stacked.counts.smooth(smoothing_radius).plot_interactive(add_cbar=True)\n",
    "\n",
    "dataset_stacked.exposure.smooth(smoothing_radius).plot_interactive(add_cbar=True)\n",
    "\n",
    "dataset_stacked.background.smooth(smoothing_radius).plot_interactive(add_cbar=True)\n",
    "\n",
    "dataset_stacked.excess.smooth(smoothing_radius).plot_interactive(\n",
    "    stretch=\"sqrt\", add_cbar=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16961708-4c87-4c3a-9aeb-1e3100f6c513",
   "metadata": {},
   "source": [
    "We can see that the center of the observation was more towards 1ES1218+304, as we can see higher values in the counts, background and exposure maps.\n",
    "And indeed, from the previous notebook, we remember that the wobbling observation was performed around 1ES1218+304's coordinates.\n",
    "We can also observe that the excess map already shows an enhanced emission from 1ES1218+304 and 1ES1215+303."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de2a98-3ff8-4514-b9ef-a872a42a9f31",
   "metadata": {},
   "source": [
    "### 4.4 Detection maps\n",
    "\n",
    "Now we can quantify the excess in the map using significance estimators. Gammapy provides two different approaches an `ExcessMapEstimator`, which estimates the source significance based on the counts similar to the 1D case or a `TSMapEstimator`, which fits a defined source model in each pixel of the map and determines the significance for a hypothetical source from the likelihood (this is the same approach the _Fermi_-LAT analysis uses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6eed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here a quick function to add the sources coordinates to the maps\n",
    "\n",
    "\n",
    "def plot_sources_coords(ax, wcs):\n",
    "    \"\"\"Add sources positions to significance and excess maps\"\"\"\n",
    "    source1_coords = PointSkyRegion(_1ES1215_coordinates)\n",
    "    source1_coords.to_pixel(wcs).plot(\n",
    "        ax=ax,\n",
    "        color=\"forestgreen\",\n",
    "        marker=\"*\",\n",
    "        ls=\"\",\n",
    "        markersize=12,\n",
    "        label=\"1ES1215+303\",\n",
    "    )\n",
    "\n",
    "    source2_coords = PointSkyRegion(_1ES1218_coordinates)\n",
    "    source2_coords.to_pixel(wcs).plot(\n",
    "        ax=ax, color=\"dodgerblue\", marker=\"*\", ls=\"\", markersize=12, label=\"1ES1218+304\"\n",
    "    )\n",
    "\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b85e9-6d81-4193-9b13-4270cc734843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to maximise the significance for a point source,\n",
    "# the correlation radius should be of the size of the PSF.\n",
    "# for extended sources try sqrt(psf**2+extension**2), which\n",
    "# assumes a Gaussian\n",
    "correlation_radius = 0.07 * u.deg\n",
    "estimator = ExcessMapEstimator(\n",
    "    correlation_radius, selection_optional=[], correlate_off=True\n",
    ")\n",
    "lima_maps = estimator.run(dataset_stacked)\n",
    "\n",
    "significance_map = lima_maps[\"sqrt_ts\"]\n",
    "excess_map = lima_maps[\"npred_excess\"]\n",
    "\n",
    "# We can plot the excess and significance maps\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    figsize=(11, 4), subplot_kw={\"projection\": lima_maps.geom.wcs}, ncols=2\n",
    ")\n",
    "ax1.set_title(\"Significance map\")\n",
    "significance_map.plot(ax=ax1, vmax=7, add_cbar=True)\n",
    "plot_sources_coords(ax1, lima_maps.geom.wcs)\n",
    "\n",
    "ax2.set_title(\"Excess map\")\n",
    "excess_map.plot(ax=ax2, add_cbar=True)\n",
    "plot_sources_coords(ax2, excess_map.geom.wcs)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88033429",
   "metadata": {},
   "source": [
    "And we can cleary see that the excesses correspond to the sources position, but to check whether the results are robust, it is worth to check the significance distribution of pixels. The background-only pixel should form a Gaussian with mean 0 and sigma 1 - at least approximately. If there are multiple sources in the FoV or your source extens over a significant fraction of the map, this assumption may not hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933c543-dad0-425d-ad4c-8d79325600ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all values of significance\n",
    "significance_all = lima_maps[\"sqrt_ts\"].data[np.isfinite(lima_maps[\"sqrt_ts\"].data)]\n",
    "# all values of significance outside exclusion regions, i.e. excluding the regions around the sources\n",
    "significance_off = lima_maps[\"sqrt_ts\"].data[\n",
    "    np.logical_and(np.isfinite(lima_maps[\"sqrt_ts\"].data), exclusion_mask.data)\n",
    "]\n",
    "bins = np.linspace(\n",
    "    np.min(significance_all),\n",
    "    np.max(significance_all),\n",
    "    num=int((np.max(significance_all) - np.min(significance_all)) * 3),\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(\n",
    "    significance_all,\n",
    "    density=True,\n",
    "    alpha=0.5,\n",
    "    color=\"red\",\n",
    "    label=\"all bins\",\n",
    "    bins=bins,\n",
    ")\n",
    "\n",
    "ax.hist(\n",
    "    significance_off,\n",
    "    density=True,\n",
    "    alpha=0.5,\n",
    "    color=\"blue\",\n",
    "    label=\"background bins\",\n",
    "    bins=bins,\n",
    ")\n",
    "\n",
    "# Now, fit the off distribution with a Gaussian\n",
    "mu, std = norm.fit(significance_off)\n",
    "x = np.linspace(-8, 8, 50)\n",
    "p = norm.pdf(x, mu, std)\n",
    "ax.plot(x, p, lw=2, color=\"black\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Significance\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylim(1e-5, 1)\n",
    "xmin, xmax = np.min(significance_all), np.max(significance_all)\n",
    "ax.set_xlim(xmin, 7)\n",
    "text = r\"$\\mu$ = {:.2f}\" f\"\\n\" r\"$\\sigma$ = {:.2f}\".format(mu, std)\n",
    "box_prop = dict(boxstyle=\"Round\", facecolor=\"white\", alpha=0.5)\n",
    "text_prop = dict(fontsize=\"x-large\", bbox=box_prop)\n",
    "txt = AnchoredText(text, loc=2, transform=ax.transAxes, prop=text_prop, frameon=False)\n",
    "ax.add_artist(txt)\n",
    "\n",
    "print(f\"Fit results: mu = {mu:.2f}, std = {std:.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6f21e-81a9-4e47-b8a3-8eedc3d087cb",
   "metadata": {},
   "source": [
    "Now we are going to use TS estimator, for which we first need to define a model for our test source. As we deal here with point sources, we choose a point source for the spatial model and for the spectrum we use a power-law for simplicity. The closer your model choice is to reality the better your results will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9f1c9-9c6a-4f2f-95ca-3498555ffbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_model = PointSpatialModel()\n",
    "# we choose units consistent with the map units here...\n",
    "spectral_model = PowerLawSpectralModel(amplitude=\"1e-22 cm-2 s-1 keV-1\", index=2)\n",
    "model = SkyModel(spatial_model=spatial_model, spectral_model=spectral_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e963b-4d1d-40b0-86df-3f7135cfbfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TSMapEstimator(\n",
    "    model,\n",
    "    kernel_width=\"0.07 deg\",\n",
    "    energy_edges=[80, 8000] * u.GeV,\n",
    ")\n",
    "maps = estimator.run(dataset_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405dc023-637d-4811-b3e2-6bece6ce6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(\n",
    "    ncols=3,\n",
    "    figsize=(15, 3),\n",
    "    subplot_kw={\"projection\": geom.wcs},\n",
    "    gridspec_kw={\"left\": 0.1, \"right\": 0.98},\n",
    ")\n",
    "\n",
    "maps[\"sqrt_ts\"].plot(ax=ax1, vmax=7, add_cbar=True)\n",
    "ax1.set_title(\"Significance map\")\n",
    "maps[\"flux\"].plot(ax=ax2, add_cbar=True, stretch=\"sqrt\", vmin=0)\n",
    "ax2.set_title(\"Flux map\")\n",
    "maps[\"niter\"].plot(ax=ax3, add_cbar=True)\n",
    "ax3.set_title(\"Iteration map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0742d-049f-4eab-9390-abe019f1f457",
   "metadata": {},
   "source": [
    "Also this procedure clearly detected the two sources. The advantage of this method is that it also provides a map in physical meaningful units, the flux map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e4d5e3-b183-47b3-ae23-2337a7b3f7ba",
   "metadata": {},
   "source": [
    "## 4.5 3D spectrum fit\n",
    "\n",
    "For the 3d fit we need to define a model for the sources. Each model consists of two components, a spatial and a spectral. The spatial component in case of the the two blazars are simple as it is a point source for the resolution of the IACTs. Hence, we only need to define the position. For the spectral model we use power-law models following the findings of 1D analyses. We then combine the components into one source model for either source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d6072-62a2-40b4-924b-6f76e0b13978",
   "metadata": {},
   "outputs": [],
   "source": [
    "_1ES1215_spatial_model = PointSpatialModel(\n",
    "    lon_0=_1ES1215_coordinates.ra, lat_0=_1ES1215_coordinates.dec, frame=\"icrs\"\n",
    ")\n",
    "\n",
    "_1ES1218_spatial_model = PointSpatialModel(\n",
    "    lon_0=_1ES1218_coordinates.ra, lat_0=_1ES1218_coordinates.dec, frame=\"icrs\"\n",
    ")\n",
    "\n",
    "# let us copy the spectral models from those we obtained with the 1D analysis\n",
    "# so we take also the EBL into account, we now add a reals spatial model\n",
    "_1ES1215_model_1d = Models.read(\"results/1ES1215+303/model_1ES1215+303.yaml\")\n",
    "_1ES1215_model_3d = _1ES1215_model_1d[0].copy()\n",
    "_1ES1215_model_3d._name = \"1ES1215+303\"\n",
    "_1ES1215_model_3d.spatial_model = _1ES1215_spatial_model\n",
    "\n",
    "_1ES1218_model_1d = Models.read(\"results/1ES1218+304/model_1ES1218+304.yaml\")\n",
    "_1ES1218_model_3d = _1ES1218_model_1d[0].copy()\n",
    "_1ES1218_model_3d._name = \"1ES1218+304\"\n",
    "_1ES1218_model_3d.spatial_model = _1ES1218_spatial_model\n",
    "\n",
    "# let us check the models now\n",
    "print(_1ES1215_model_3d)\n",
    "print(_1ES1218_model_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c932d-a5c6-46e3-afd2-1f95be7efca6",
   "metadata": {},
   "source": [
    "And we now see that also the coordinates have become parameters of the model.\n",
    "\n",
    "For the fit to perform better, we can constrain the values of each parameter to reasonable ranges. \n",
    "It will deliver better results and makes the fit converging faster, but requires that these ranges are known apriori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d4f88-49ec-4eda-93c9-06a8b1e39ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we actually freeze the sources coordinates\n",
    "_1ES1215_model_3d.parameters[\"lon_0\"].frozen = True\n",
    "_1ES1215_model_3d.parameters[\"lat_0\"].frozen = True\n",
    "\n",
    "_1ES1218_model_3d.parameters[\"lon_0\"].frozen = True\n",
    "_1ES1218_model_3d.parameters[\"lat_0\"].frozen = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9992eba-e573-4b30-aa86-eac680442afb",
   "metadata": {},
   "source": [
    "We now can define the energy range in true energy in which we like to fit the model. This does not agree with the energy range in which we extract the data and irfs. As a rule of thumb the true energy range can be a bit wider than the range in reconstructed energy. Here in addition we define the source region, in which case we only fit the source.\n",
    "\n",
    "Alternatively, we can only define the energy mask and then fit the source and background together across the FoV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b20a641-f6d8-4da5-8689-5dc8b568ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_mask = dataset_stacked.counts.geom.energy_mask(\n",
    "    energy_min=0.08 * u.TeV, energy_max=20 * u.TeV\n",
    ")\n",
    "dataset_stacked.mask_fit = energy_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4aa980-79cd-4353-9361-aab0d3e1b973",
   "metadata": {},
   "source": [
    "### 4.5.2. Adjusting the Background model\n",
    "\n",
    "In case we fit also the background we need to define a model for it as well. If we do not fit the background, the background model is just subtrackted and the source is fit on the excess map. \n",
    "\n",
    "The spatial model is given by the background model. For the spectrum, we can either assume that the background is scaled correctly across the bins and just apply a single scaling factor across all energy bins or we allow a different normalisation factor for each energy bin. The former is faster, but the latter should be more precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b6411e-04a5-404a-8ca6-6d409c708fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_center = dataset_stacked.geoms[\"geom\"].axes[\"energy\"].center\n",
    "\n",
    "spectral_model_bkg = PiecewiseNormSpectralModel(\n",
    "    energy=energy_center,\n",
    "    norms=np.ones(energy_center.shape),\n",
    ")\n",
    "bkg_model = FoVBackgroundModel(\n",
    "    dataset_name=\"stacked\", spectral_model=spectral_model_bkg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9701507-83bf-419b-9e77-a54a24597614",
   "metadata": {},
   "source": [
    "Now we define the final model to be fit to the data. In case we restricted the spatial fit region, one should better only use the source model. In case you use the entire FoV, the background can be fit as well. However, the fit of the background performed earlier should have already sufficiently optimized the background enough in this case. In case you have a complex region with several sources, we recommend to fit the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3021bc3-d227-4cf5-9eca-fa3934975867",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_model_3d = Models([_1ES1218_model_3d, _1ES1215_model_3d, bkg_model])\n",
    "dataset_stacked.models = total_model_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7788bf47-4d1f-4cc2-8f8c-a552936fe322",
   "metadata": {},
   "source": [
    "Finally we can run the fit using the minuit minimizer. We can defint he output level and settings for the minimizer. We save the outcome of the fit to a seperate object as well as the report from the minimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20649e-7138-4643-bba3-690628f5a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = Fit(optimize_opts={\"tol\": 0.01, \"strategy\": 2, \"print_level\": 0})\n",
    "result = fit.run(datasets=dataset_stacked)\n",
    "minuit_result = result.optimize_result.minuit\n",
    "\n",
    "print(result)\n",
    "print(minuit_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161cfc58-7c0e-4a99-a2d8-303a1157850f",
   "metadata": {},
   "source": [
    "Please always check whether the fit finished successfully. If a fit fails, please check whether the starting parameters of your input model are reasonable. It can also improve the stability of the fit to freeze or constrain some parameters. For better readability, we can also print the final results for the source parameters in separate table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28568c6a-50b5-4f8b-8a8a-2edd94a95a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(total_model_3d.to_parameters_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a16d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute flux points\n",
    "energy_edges = energy_axis.edges[5:13]\n",
    "flux_points_estimator_1ES1215 = FluxPointsEstimator(\n",
    "    energy_edges=energy_edges, source=\"1ES1215+303\", selection_optional=\"all\"\n",
    ")\n",
    "flux_points_1ES1215_3d = flux_points_estimator_1ES1215.run(datasets=dataset_stacked)\n",
    "\n",
    "energy_edges = energy_axis.edges[4:14]\n",
    "flux_points_estimator_1ES1218 = FluxPointsEstimator(\n",
    "    energy_edges=energy_edges, source=\"1ES1218+304\", selection_optional=\"all\"\n",
    ")\n",
    "flux_points_1ES1218_3d = flux_points_estimator_1ES1218.run(datasets=dataset_stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57487170-3fed-48e2-8c07-9f43a230a7c8",
   "metadata": {},
   "source": [
    "### 4.5.3. Checking the fit results\n",
    "\n",
    "To check the agreement between the fit model and the data, we should check the residuals in the source region. If significant residuals remain the model does not describe the data very well. In our case we see that the difference between model and data are within 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae6080-0f50-47bf-9fba-c7a1b2a3b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stacked.plot_residuals_spatial(method=\"diff/sqrt(model)\", vmin=-0.5, vmax=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a5bda-607a-4ec9-85e5-2080bda49bd6",
   "metadata": {},
   "source": [
    "For each source region one can also the data model agreement across the different energy bins to see in which energy bins one has the biggest descripancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b716a-e376-401a-be35-f954c76c0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = CircleSkyRegion(_1ES1215_model_3d.position, radius=0.15 * u.deg)\n",
    "dataset_stacked.plot_residuals(\n",
    "    kwargs_spatial=dict(method=\"diff/sqrt(model)\", vmin=-1, vmax=1),\n",
    "    kwargs_spectral=dict(region=region),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aeb27d-ed18-4af5-b0f8-174a18e9ae06",
   "metadata": {},
   "source": [
    "We can further study the spatial structure of the residual in different energy bins using the excess map estimator of gammapy again. This will show us significant residuals over our best fit model in the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4e2c5-2cd8-4c04-b4e6-32b9fca14535",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = ExcessMapEstimator(\n",
    "    correlation_radius=\"0.05 deg\",\n",
    "    selection_optional=[],\n",
    "    energy_edges=[0.1, 1, 10] * u.TeV,\n",
    ")\n",
    "\n",
    "result = estimator.run(dataset_stacked)\n",
    "result[\"sqrt_ts\"].plot_grid(\n",
    "    figsize=(12, 4), cmap=\"coolwarm\", add_cbar=True, vmin=-5, vmax=5, ncols=2\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15139005-6d4d-471a-a570-d1d3416e1690",
   "metadata": {},
   "source": [
    "For low energies, we see an positive excess at the position of 1ES1218. This can happen for bright sources at low energies as the Monte-Carlo based PSF does not describe the tails in the real data well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ae2658-3d1f-4c1d-b81b-50d1f7582f9c",
   "metadata": {},
   "source": [
    "## 4.6. Comparison of 1D vs 3D results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf773cc7-403c-4866-8223-8e36beb359d5",
   "metadata": {},
   "source": [
    "In addition to the spectral fit, we can estimate flux data points from the data.   \n",
    "It would be most interesting to compare the results of the 1D and 3D analyses. Even if we used different data reductions, and different background estimation methods, they should agree within the uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 1D flux points for comparison\n",
    "flux_points_1ES1218_1d = FluxPoints.read(\n",
    "    \"results/1ES1218+304/flux_points_1ES1218+304.fits\"\n",
    ")\n",
    "flux_points_1ES1215_1d = FluxPoints.read(\n",
    "    \"results/1ES1215+303/flux_points_1ES1215+303.fits\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a3f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plot_gammapy_sed(\n",
    "    ax,\n",
    "    _1ES1218_model_1d[\"1ES1218+304\"].spectral_model,\n",
    "    flux_points_1ES1218_1d,\n",
    "    \"crimson\",\n",
    "    \"1ES1218+304, 1D analysis\",\n",
    ")\n",
    "plot_gammapy_sed(\n",
    "    ax,\n",
    "    total_model_3d[\"1ES1218+304\"].spectral_model,\n",
    "    flux_points_1ES1218_3d,\n",
    "    \"rosybrown\",\n",
    "    \"1ES1218+304, 3D analysis\",\n",
    ")\n",
    "\n",
    "plot_gammapy_sed(\n",
    "    ax,\n",
    "    _1ES1215_model_1d[\"1ES1215+303\"].spectral_model,\n",
    "    flux_points_1ES1215_1d,\n",
    "    \"dodgerblue\",\n",
    "    \"1ES1215+303, 1D analysis\",\n",
    ")\n",
    "plot_gammapy_sed(\n",
    "    ax,\n",
    "    total_model_3d[\"1ES1215+303\"].spectral_model,\n",
    "    flux_points_1ES1215_3d,\n",
    "    \"lightblue\",\n",
    "    \"1ES1215+303, 3D analysis\",\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlim([60, 2e4])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gammapy-1.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
