{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a06de0c",
   "metadata": {},
   "source": [
    "# 3. Estimation of the flux of a variable source, Mrk421\n",
    "\n",
    "## 3.1 Context\n",
    "In the first two introductory notebooks, we considered Crab Nebula for its property of being a **bright** and **steady** emitter in the gamma-ray sky. Let us examine now a bright source whose flux is not constant in time: the active galactic nucleus Mrk421.   \n",
    "We will consider the state of intense gamma-ray activity measured in April 2013 by the MAGIC Collaboration, whose analysis has been published in [MAGIC Collaboration 2020](https://ui.adsabs.harvard.edu/abs/2020ApJS..248...29A/abstract)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d149e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - basic imports (numpy, astropy, regions, matplotlib)\n",
    "import operator\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from astropy.time import Time\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.io.fits.verify import VerifyWarning\n",
    "from regions import PointSkyRegion, CircleSkyRegion\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# - Gammapy's imports\n",
    "from gammapy.maps import Map, MapAxis, WcsGeom, RegionGeom\n",
    "from gammapy.data import DataStore, Observation\n",
    "from gammapy.datasets import SpectrumDataset, Datasets, FluxPointsDataset\n",
    "from gammapy.makers import (\n",
    "    SpectrumDatasetMaker,\n",
    "    WobbleRegionsFinder,\n",
    "    ReflectedRegionsBackgroundMaker,\n",
    ")\n",
    "from gammapy.modeling import Parameter\n",
    "from gammapy.modeling.models import (\n",
    "    PowerLawSpectralModel,\n",
    "    LogParabolaSpectralModel,\n",
    "    ConstantSpectralModel,\n",
    "    SkyModel,\n",
    "    TemporalModel,\n",
    "    LinearTemporalModel,\n",
    ")\n",
    "from gammapy.modeling import Fit\n",
    "from gammapy.estimators import FluxPointsEstimator, LightCurveEstimator\n",
    "from gammapy.stats import WStatCountsStatistic\n",
    "\n",
    "# - setting up logging and ignoring warnings\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", VerifyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6828c1d0",
   "metadata": {},
   "source": [
    "## 3.2. Data Reduction\n",
    "As before, let us load all the Mrk421 observations and reduce the data.   \n",
    "Let us focus on the days between the 10th and the 20th of April, where the most intense activity was recorded, by using `Gammapy's` `Observations.select_time` to select these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3cb7ed-75d4-4b7f-9109-170ec8dee5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load observations\n",
    "datastore = DataStore.from_dir(\"../acme_magic_odas_data/data/Mrk421\")\n",
    "observations = datastore.get_observations(required_irf=[\"rad_max\", \"aeff\", \"edisp\"])\n",
    "print(f\"total observations : {len(observations)}\")\n",
    "\n",
    "# select observations between 10th and 20th of April\n",
    "times = [\"2013-04-10T12:00:00\", \"2013-04-20T12:00:00\"]\n",
    "time_intervals = Time(times, format=\"isot\", scale=\"utc\")\n",
    "observations = observations.select_time(time_intervals)\n",
    "print(f\"selected observations : {len(observations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1403bb69-78c8-4751-92a7-b23a912d9abd",
   "metadata": {},
   "source": [
    "As you noticed, we used times from noon to noon. That's just because, as most of the observations happen over night (typically between 20:00 and 06:00, of course depending on the season), that's a safe way of ensuring we are selecting all the observations in a given night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us define the parameters of the spectrum extraction\n",
    "# - we need to specify only the center of the on region,\n",
    "# its radius will be fetched from the RAD_MAX table.\n",
    "crab_coords = SkyCoord.from_name(\"Mrk421\")\n",
    "on_region = PointSkyRegion(crab_coords)\n",
    "\n",
    "# - let us define the energy axes over which we want:\n",
    "# -- to bin the counts (estimated energies) and\n",
    "energy_axis = MapAxis.from_energy_bounds(\n",
    "    10, 1e5, nbin=20, per_decade=False, unit=\"GeV\", name=\"energy\"\n",
    ")\n",
    "# -- to interpolate the IRFs (true energies)\n",
    "energy_axis_true = MapAxis.from_energy_bounds(\n",
    "    10, 1e5, nbin=28, per_decade=False, unit=\"GeV\", name=\"energy_true\"\n",
    ")\n",
    "\n",
    "# let us create an empty dataset with this spatial and energy structure / binning\n",
    "geom = RegionGeom.create(region=on_region, axes=[energy_axis])\n",
    "dataset_empty = SpectrumDataset.create(geom=geom, energy_axis_true=energy_axis_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d62951-d1b0-43e4-a444-23f8d5d77c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let us repeat the process of data reduction already illustrated in the previous notebook\n",
    "dataset_maker = SpectrumDatasetMaker(\n",
    "    containment_correction=False, selection=[\"counts\", \"exposure\", \"edisp\"]\n",
    ")\n",
    "# use 3 off regions to estimate the background\n",
    "region_finder = WobbleRegionsFinder(n_off_regions=3)\n",
    "bkg_maker = ReflectedRegionsBackgroundMaker(region_finder=region_finder)\n",
    "\n",
    "datasets = Datasets()\n",
    "\n",
    "for observation in observations:\n",
    "    dataset = dataset_maker.run(\n",
    "        dataset_empty.copy(name=str(observation.obs_id)), observation\n",
    "    )\n",
    "    dataset_on_off = bkg_maker.run(dataset, observation)\n",
    "    datasets.append(dataset_on_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee896b7d-6205-4137-8f81-0a323419ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us just take a look at one of the observations to be sure that everything is all right\n",
    "datasets[0].peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2745d60-befe-4d60-84d2-4a8c43b50d18",
   "metadata": {},
   "source": [
    "## 3.3. Light curve: integral flux vs time\n",
    "\n",
    "We are now concerned with studying the **evolution of the flux in time**: How could we measure it?      \n",
    "One option would certainly be to perform the spectrum estimation (demostrated in the previous notebook) per each of the observations (or per day) separately. In this way we could measure spectral changes run by run (or day by day). Let us keep this option in the back of our mind. \n",
    "\n",
    "To measure variability in astronomy it is common practice to compile a _light curve_, a graph of the light intensity or brightness as a function of time. To represent the brightness of a source in gamma rays we will consider the **integral flux** above a certain energy, $E_{\\rm thr}$\n",
    "\n",
    "$$\n",
    "    \\phi(E > E_{\\rm thr})\\,[{\\rm cm}^{-2}\\,{\\rm s}^{-1}] = \n",
    "    \\int_{E_{\\rm thr}}^{\\infty} \\frac{{\\rm d}\\phi}{{\\rm d}E}(E; \\hat{\\theta})\\,{\\rm d}E\n",
    "$$\n",
    "\n",
    "where $\\hat{\\theta}$ represents the parameters of the differential flux model $\\frac{{\\rm d}\\phi}{{\\rm d}E}$ we adjusted to the data. $E_{\\rm thr}$, the extreme of integration, can depend on the analysis, or on the physics we are interested in. One can in principle perform the likelihood fit of the previous notebook in a given time interval (might be an observation, a day, a week) to determine $\\frac{{\\rm d}\\phi}{{\\rm d}E}(E; \\hat{\\theta})$, and then simply integrate the spectrum obtained.\n",
    "\n",
    "It is more common practice to fix the parameters of the spectrum adopted $\\hat{\\theta}$, except for the normalisation $\\phi_0$. This is very similar to what we did in the flux point computation. In that case, we re-adjusted the best-fit value for $\\phi_0$ using _all the events within an estimated energy bin_. We now re-adjust it using _all the events in a single time interval_.\n",
    "Given the parallel between the processes of flux points and light curve estimation, `Gammapy` provides a `LightCurveEstimator`  that works very simlarly to the `FluxPointsEstimator`.\n",
    "\n",
    "Let us then compute the light curve for Mrk421. Which model should we assume for the likelihood fitting? In the paper presenting this dataset ([MAGIC Collaboration, 2020](https://ui.adsabs.harvard.edu/abs/2020ApJS..248...29A/abstract)) we read:\n",
    "\n",
    "\n",
    "> [...] the VHE gamma-ray spectrum from the full nine day data set [...] is well represented by the following log-parabola function:    \n",
    "> $$ \\frac{{\\rm d}\\phi}{{\\rm d}E} = \\phi_0 \\left( \\frac{E}{0.3\\,{\\rm TeV}} \\right)^{- 2.14 - 0.45 \\log_{10}(\\frac{E}{0.3\\,{\\rm TeV}})} $$\n",
    "\n",
    "The amplitude parameter is not specified. Let us see if we can recover the same value fitting all the data together, let us stack and fit them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c60787e-f663-496f-a1dc-9ec48633aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stacked = datasets.stack_reduce()\n",
    "\n",
    "# let us use the LP model, use the same reference energy of the paper\n",
    "total_spectral_model = LogParabolaSpectralModel(\n",
    "    amplitude=5e-12 * u.Unit(\"TeV-1 cm-2 s-1\"),\n",
    "    reference=0.3 * u.TeV,\n",
    "    alpha=2.3 * u.Unit(\"\"),\n",
    "    beta=0.1 * u.Unit(\"\"),\n",
    ")\n",
    "total_model = SkyModel(spectral_model=total_spectral_model, name=\"Mrk421\")\n",
    "\n",
    "# let us use a reasonable energy range for fitting\n",
    "e_min = 0.08 * u.TeV\n",
    "e_max = 10 * u.TeV\n",
    "dataset_stacked.counts.geom.energy_mask(e_min, e_max)\n",
    "\n",
    "# assign the model to the\n",
    "dataset_stacked.models = [total_model]\n",
    "\n",
    "# run the fit!\n",
    "fit = Fit()\n",
    "results = fit.run(datasets=dataset_stacked)\n",
    "print(results)\n",
    "print(total_spectral_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20853fff-266c-4e58-8037-654a06fbd2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"alpha = {total_spectral_model.alpha.value:.2f} +/- {total_spectral_model.alpha.error:.2f}\"\n",
    ")\n",
    "# NOTE: the log in Gammapy's log parabola is in base e\n",
    "# the one in the paper is in base 10, make the conversion\n",
    "beta = total_spectral_model.beta.value / np.log10(np.e)\n",
    "beta_err = total_spectral_model.beta.error / np.log10(np.e)\n",
    "print(f\"beta = {beta:.2f} +/- {beta_err:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff6a784-60c3-4b72-99d8-029b50685569",
   "metadata": {},
   "source": [
    "We, obtained values quite similar to the average spectrum presented in the paper. Let us go ahead and compute the LC with the `LightCurveEstimator`. If no time interval is specified, then one flux point will be estimated for each observation. Let us compute the light curve above 1 TeV, as done in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac89ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# energy range in which the integral flux has to be computed\n",
    "energy_edges = [1 * u.TeV, 100 * u.TeV]\n",
    "\n",
    "# freeze the parameters of the spectral model\n",
    "# only the norm has to be refitted\n",
    "total_spectral_model.alpha.frozen = True\n",
    "total_spectral_model.beta.frozen = True\n",
    "\n",
    "# rember to assing the model to the datasets\n",
    "datasets.models = [total_model]\n",
    "\n",
    "light_curve_estimator_run_wise = LightCurveEstimator(\n",
    "    energy_edges=energy_edges,\n",
    "    source=\"Mrk421\",\n",
    "    reoptimize=False,\n",
    "    n_sigma_ul=3,\n",
    ")\n",
    "\n",
    "light_curve_run_wise = light_curve_estimator_run_wise.run(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e57ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us plot the result\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "light_curve_run_wise.plot(\n",
    "    ax=ax, sed_type=\"flux\", marker=\".\", label=r\"run-wise time bins\"\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_ylabel(r\"$\\Phi(E > 1\\,{\\rm TeV})$\")\n",
    "ax.set_yscale(\"linear\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e4b5ab-6c3b-496b-9d37-8e00c691e8bf",
   "metadata": {},
   "source": [
    "It is common practice in astronomy to represent time in [Modified Julian Date (MJD)](https://en.wikipedia.org/wiki/Julian_day) to have a continuous count of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c9994d-ba63-4884-9556-74ba1829a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "light_curve_run_wise.plot(\n",
    "    ax=ax,\n",
    "    sed_type=\"flux\",\n",
    "    time_format=\"mjd\",\n",
    "    marker=\".\",\n",
    "    label=r\"run-wise time bins\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_ylabel(r\"$\\Phi(E > 1\\,{\\rm TeV})\\,/\\,({\\rm cm}^{-2}\\,{\\rm s}^{-1})$\")\n",
    "ax.set_yscale(\"linear\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5355e935",
   "metadata": {},
   "source": [
    "### Exercise 3.1.\n",
    "Compute the light curve of Mrk 421 assuming a power law spectral model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a72915-9060-430c-adfa-ed8ca4424544",
   "metadata": {},
   "source": [
    "\n",
    "## 3.4. Custom time binning\n",
    "Without specifying any time intervals for the `LightCurveEstimator`, the duration of each run was used for the bins in energy.   \n",
    "Let us now adopt a custom time binning: let us compute the daily (or, more properly, nightly) integral fluxes. We will use MJD to define the time intervals and again the center of a bin is considered to be at midnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c0efa-e170-4bbd-befa-fba46a0c29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_time_intervals = [\n",
    "    Time([56392.5, 56393.5], format=\"mjd\", scale=\"utc\"),\n",
    "    Time([56393.5, 56394.5], format=\"mjd\", scale=\"utc\"),\n",
    "    Time([56394.5, 56395.5], format=\"mjd\", scale=\"utc\"),\n",
    "    Time([56395.5, 56396.5], format=\"mjd\", scale=\"utc\"),\n",
    "    Time([56396.5, 56397.5], format=\"mjd\", scale=\"utc\"),\n",
    "    Time([56397.5, 56398.5], format=\"mjd\", scale=\"utc\"),\n",
    "    Time([56398.5, 56399.5], format=\"mjd\", scale=\"utc\"),\n",
    "    Time([56399.5, 56400.5], format=\"mjd\", scale=\"utc\"),\n",
    "    Time([56400.5, 56401.5], format=\"mjd\", scale=\"utc\"),\n",
    "]\n",
    "\n",
    "light_curve_estimator_daily = LightCurveEstimator(\n",
    "    energy_edges=energy_edges,\n",
    "    time_intervals=daily_time_intervals,\n",
    "    source=\"Mrk421\",\n",
    "    reoptimize=False,\n",
    "    n_sigma_ul=3,\n",
    ")\n",
    "light_curve_daily = light_curve_estimator_daily.run(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb9c61-4d5b-454a-85e5-e9c3335e4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us now plot both the daily and run-wise light curves\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "light_curve_run_wise.plot(\n",
    "    ax=ax,\n",
    "    sed_type=\"flux\",\n",
    "    time_format=\"mjd\",\n",
    "    marker=\".\",\n",
    "    label=\"run-wise binning\",\n",
    "    alpha=0.6,\n",
    ")\n",
    "light_curve_daily.plot(\n",
    "    ax=ax,\n",
    "    sed_type=\"flux\",\n",
    "    time_format=\"mjd\",\n",
    "    marker=\",\",\n",
    "    label=\"nightly binning\",\n",
    "    elinewidth=2,\n",
    ")\n",
    "ax.set_ylabel(r\"$\\Phi(E > 1\\,{\\rm TeV})\\,/\\,({\\rm cm}^{-2}\\,{\\rm s}^{-1})$\")\n",
    "ax.set_yscale(\"linear\")\n",
    "ax.set_ylim([0, 4e-10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177d4e3-af30-4ef1-8dbf-06dca2efe9c4",
   "metadata": {},
   "source": [
    "## 3.5. Light curve: spectral parameters vs time\n",
    "\n",
    "For the integral flux estimation we froze all the spectral parameters except for the amplitude. This type of light curve shows us only the overall flux variation, without displaying the change in spectral parameters. It is common practice in high-energy astronomy (whenever we can measure a spectrum) to make a plot of the spectral parameters vs time. Let us use a power-law, for simplicty, and let us compute the daily amplitudes and spectral indexes. We will now use the function `select_time` but for the `Datasets`, to select and fit those in a single day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a5bf26-30c8-4bed-a224-d6b908c32e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us create a list of Spectral Models\n",
    "spectral_models = []\n",
    "for i in range(len(daily_time_intervals)):\n",
    "    _pwl = PowerLawSpectralModel(reference=0.3 * u.TeV)\n",
    "    spectral_models.append(_pwl)\n",
    "\n",
    "# define the fitter\n",
    "fit = Fit()\n",
    "\n",
    "# let us select a sub-dataset for each day, and\n",
    "for daily_time_interval, spectral_model in zip(daily_time_intervals, spectral_models):\n",
    "    # select the dataset in a given time\n",
    "    daily_datasets = datasets.select_time(*daily_time_interval)\n",
    "    print(f\"{len(daily_datasets)} selected for {daily_time_interval}\")\n",
    "    # apply proper fitting range to all obs. selected\n",
    "    for _dataset in daily_datasets:\n",
    "        _dataset.counts.geom.energy_mask(e_min, e_max)\n",
    "    # define model and assign it\n",
    "    model = SkyModel(spectral_model=spectral_model, name=\"Mrk421\")\n",
    "    daily_datasets.models = [model]\n",
    "    # fit\n",
    "    results = fit.run(datasets=daily_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57050ad5-5e9e-46f5-a8c6-2ed326189e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let us make a plot of the spectral parameters\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 5), sharex=True, gridspec_kw={\"hspace\": 0.05})\n",
    "\n",
    "# the values of the spectral models should have been automatically updated in the list\n",
    "# of spectral models\n",
    "for daily_time_interval, spectral_model in zip(\n",
    "    daily_time_intervals[1:-1], spectral_models\n",
    "):\n",
    "    time_bin_center = 0.5 * (daily_time_interval[0].mjd + daily_time_interval[1].mjd)\n",
    "    time_bin_width = 0.5 * (daily_time_interval[1].mjd - daily_time_interval[0].mjd)\n",
    "    ax[0].errorbar(\n",
    "        time_bin_center - 56392,\n",
    "        spectral_model.amplitude.value / 1e-9,\n",
    "        xerr=[time_bin_width],\n",
    "        yerr=spectral_model.amplitude.error / 1e-9,\n",
    "        marker=\".\",\n",
    "        ls=\"\",\n",
    "        color=\"k\",\n",
    "    )\n",
    "    ax[1].errorbar(\n",
    "        time_bin_center - 56392,\n",
    "        spectral_model.index.value,\n",
    "        xerr=[time_bin_width],\n",
    "        yerr=spectral_model.index.error,\n",
    "        marker=\".\",\n",
    "        ls=\"\",\n",
    "        color=\"k\",\n",
    "    )\n",
    "\n",
    "ax[0].set_ylabel(\n",
    "    r\"$\\phi\\,/\\,(10^{-9}\\,{\\rm TeV}^{-1}\\,{\\rm cm}^{-1}\\,{\\rm s}^{-1})$\", fontsize=10\n",
    ")\n",
    "ax[1].set_ylabel(r\"$\\Gamma$\")\n",
    "ax[1].set_xlabel(r\"$({\\rm MJD} - 56392)\\,/\\,{\\rm day}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8635fb04-be6d-40fa-86aa-59bbd5785aa0",
   "metadata": {},
   "source": [
    "## 3.5.1 Harder when brighter\n",
    "\n",
    "In X-ray as well as gamma-ray astronomy, the \"harder when brighter\" behavior refers to the phenomenon where the spectrum becomes harder ($< 2$) as the source brightness increases. Observing this type of behaviour gives us relevant information about the acceleration mechanisms of the source. We can check if Mrk 421 shows this behaviour, by making a plot of the integral flux against the spectral index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b13ff-0323-42a2-a067-00f0f03973aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "integral_flux_daily = light_curve_daily.flux.data.flatten()\n",
    "integral_flux_err_daily = light_curve_daily.flux_err.data.flatten()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for integral_flux, integral_flux_err, spectral_model in zip(\n",
    "    integral_flux_daily, integral_flux_err_daily, spectral_models\n",
    "):\n",
    "    ax.errorbar(\n",
    "        integral_flux,\n",
    "        spectral_model.index.value,\n",
    "        xerr=[integral_flux_err],\n",
    "        yerr=[spectral_model.index.error],\n",
    "        marker=\".\",\n",
    "        color=\"crimson\",\n",
    "    )\n",
    "plt.gca().invert_yaxis()\n",
    "ax.set_xlabel(r\"$\\Phi(E > 1\\,{\\rm TeV})\\,/\\,({\\rm cm}^{-2}\\,{\\rm s}^{-1})$\")\n",
    "ax.set_ylabel(r\"$\\Gamma$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8598baaf-100b-4dff-ab2d-dc14eab2510a",
   "metadata": {},
   "source": [
    "We indeed observe this correlation.\n",
    "\n",
    "## 3.6. Light curve: intra-run variability\n",
    "Going back to the light curve, we can observe that the flare on 13th of April is the one showing not only the highest flux, but also the highest variability. In such cases (very high flux and variability), one might want to check for variability (and thus check the light curve) on even smaller scales than that of a single observational run (typically 20-30 minutes for MAGIC). Let us focus on the 13th of April and let us compute a LC with 5-minute binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d489d-1505-4c51-a9fc-9a6ba85a33dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "flare_day = [\n",
    "    Time(56394.5, format=\"mjd\", scale=\"utc\"),\n",
    "    Time(56395.5, format=\"mjd\", scale=\"utc\"),\n",
    "]\n",
    "dataset_flare = datasets.select_time(*flare_day)\n",
    "\n",
    "# let us compute the run-wise LC for just this day\n",
    "# let us assume a log-parabolic spectrum\n",
    "dataset_flare.models = [total_model]\n",
    "# we already have defined a LC estimator with run-wise time binning\n",
    "light_curve_run_wise_flare = light_curve_estimator_run_wise.run(dataset_flare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc390e-4479-462d-9c02-bcfd6f76b378",
   "metadata": {},
   "source": [
    "Now let us divide the day in 5 minutes intervals.\n",
    "\n",
    "**Warning**: in case the time binning is smaller than the run (and hence `Observation` and `Dataset` duration), one must go back to the observations and cut them into smaller times, before creating new data sets, reducing and fitting them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019558ab-ec1a-4514-a6a2-ec5f7d53265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this cell can take a few moments\n",
    "duration = 5 * u.min\n",
    "\n",
    "# let us split the day in 10 minutes interval\n",
    "time_intervals_5min = [flare_day[0]]\n",
    "\n",
    "while time_intervals_5min[-1] <= flare_day[1]:\n",
    "    time_intervals_5min.append(time_intervals_5min[-1] + duration)\n",
    "\n",
    "time_intervals_5min = [\n",
    "    Time([tstart, tstop])\n",
    "    for tstart, tstop in zip(time_intervals_5min[:-1], time_intervals_5min[1:])\n",
    "]\n",
    "\n",
    "# and now cut the observations in these time intervals\n",
    "short_observations = observations.select_time(time_intervals_5min)\n",
    "# check that observations have been filtered\n",
    "print(f\"observations after time filtering: {len(short_observations)}\")\n",
    "print(short_observations[1].gti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb4437b-50cd-41c2-8c01-d4d5322306ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make datasets out of the new observations\n",
    "short_datasets = Datasets()\n",
    "\n",
    "for observation in short_observations:\n",
    "    dataset = dataset_maker.run(dataset_empty.copy(), observation)\n",
    "    dataset_on_off = bkg_maker.run(dataset, observation)\n",
    "    short_datasets.append(dataset_on_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01f208-c226-42cb-9525-15c1777afc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the log parabola model and compute the LC\n",
    "short_datasets.models = [total_model]\n",
    "\n",
    "light_curve_estimator_5min = LightCurveEstimator(\n",
    "    energy_edges=energy_edges,\n",
    "    time_intervals=time_intervals_5min,\n",
    "    source=\"Mrk421\",\n",
    "    reoptimize=False,\n",
    "    n_sigma_ul=3,\n",
    ")\n",
    "light_curve_5min_flare = light_curve_estimator_5min.run(short_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1858835-9d52-4ee1-8974-fe07b40b65fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "light_curve_run_wise_flare.plot(\n",
    "    ax=ax,\n",
    "    sed_type=\"flux\",\n",
    "    time_format=\"mjd\",\n",
    "    marker=\".\",\n",
    "    label=\"run-wise binning\",\n",
    "    alpha=0.6,\n",
    ")\n",
    "light_curve_5min_flare.plot(\n",
    "    ax=ax,\n",
    "    sed_type=\"flux\",\n",
    "    time_format=\"mjd\",\n",
    "    marker=\".\",\n",
    "    label=\"5-min binning\",\n",
    ")\n",
    "# ax.set_xlim([56394.8, 56395.2])\n",
    "ax.set_yscale(\"linear\")\n",
    "ax.set_ylim([0, 5e-10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f1f2c-fe95-45e6-aa27-fc0bb23c002c",
   "metadata": {},
   "source": [
    "**Note**: this is just an illustrative example. Be very careful on how you bin your LC, deciding on the optimal binning is a delicate task. \n",
    "\n",
    "## 3.7. Light curve: fitting the light curve\n",
    "\n",
    "Let us now use `Gammapy`'s temporal models to fit the flux points of the LC. Let us start with a simple linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44515e3-e961-4fd2-832b-be64c892a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets by iterating over the returned lightcurve\n",
    "lc_fp_dataset = FluxPointsDataset(data=light_curve_run_wise_flare, name=\"dataset_lc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89997dca-96ec-4430-b950-fc21de2ea096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us use, define the temporal model, midnight of that day is t_ref\n",
    "linear_time_model = LinearTemporalModel(alpha=2, beta=5 / u.d, t_ref=56395 * u.d)\n",
    "linear_time_model.alpha.frozen = True\n",
    "# let also add a constant spectral model and let us fit\n",
    "spectral_model = ConstantSpectralModel(const=1e-10 * u.Unit(\"TeV-1 cm-2 s-1\"))\n",
    "\n",
    "lc_model = SkyModel(\n",
    "    spectral_model=spectral_model,\n",
    "    temporal_model=linear_time_model,\n",
    "    name=\"time_model\",\n",
    ")\n",
    "\n",
    "lc_fp_dataset.models = lc_model\n",
    "print(lc_fp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ef9cc-ff85-4915-bf92-01a0c39f2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = Fit()\n",
    "result = fit.run(lc_fp_dataset)\n",
    "print(results)\n",
    "display(result.parameters.to_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfed7d7d-fdca-4292-b1c5-ad25cd45b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_fp_dataset.plot_spectrum(axis_name=\"time\")\n",
    "plt.yscale(\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6533fb0-5e52-4711-a6ab-c0ddcd4bf7b1",
   "metadata": {},
   "source": [
    "Let us define a custom model to add a Gaussian to try to reproduce the higher bump in the flare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76617b25-13d4-456c-8481-88d0e67c0d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPlusLinearTemporalModel(TemporalModel):\n",
    "    \"\"\"A Gaussian temporal model superimposed to a linear one\"\"\"\n",
    "\n",
    "    tag = [\"GaussianPlusLinearTemporalModel\", \"gauss_lin\"]\n",
    "\n",
    "    _t_ref_default = Time(\"2000-01-01\")\n",
    "    t_ref = Parameter(\"t_ref\", _t_ref_default.mjd, unit=\"day\", frozen=False)\n",
    "    # line parameters\n",
    "    alpha = Parameter(\"alpha\", 1.0, frozen=False)\n",
    "    beta = Parameter(\"beta\", 0.0, unit=\"d-1\", frozen=False)\n",
    "    # Gaussian parameters\n",
    "    amplitude = Parameter(\"amplitude\", 1.0, frozen=False)\n",
    "    sigma = Parameter(\"sigma\", \"2 h\", frozen=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(time, t_ref, alpha, beta, amplitude, sigma):\n",
    "        line = alpha + beta * (time - t_ref)\n",
    "        gauss = amplitude * np.exp(-((time - t_ref) ** 2) / (2 * sigma**2))\n",
    "        return line + gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5c967-c9c4-4fbc-9305-a85f028b44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us assume the reference is the peak of the flare\n",
    "t_peak = Time(\"2013-04-13T02:20:00\")\n",
    "gauss_line_temporal_model = GaussianPlusLinearTemporalModel(\n",
    "    t_ref=t_peak.mjd * u.d, sigma=0.3 * u.h, alpha=2.5, amplitude=2, beta=5 / u.d\n",
    ")\n",
    "gauss_line_temporal_model.t_ref.frozen = True\n",
    "gauss_line_temporal_model.alpha.frozen = True\n",
    "\n",
    "lc_model2 = SkyModel(\n",
    "    spectral_model=spectral_model,\n",
    "    temporal_model=gauss_line_temporal_model,\n",
    "    name=\"time_model2\",\n",
    ")\n",
    "# assign this new model to the flux points of the LC\n",
    "lc_fp_dataset.models = lc_model2\n",
    "\n",
    "# check the initial model before fitting\n",
    "lc_fp_dataset.plot_spectrum(axis_name=\"time\")\n",
    "plt.yscale(\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf0061d-0848-4597-9722-c440e99322d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = Fit()\n",
    "result = fit.run(lc_fp_dataset)\n",
    "print(results)\n",
    "display(result.parameters.to_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b803c-aff7-4b8b-b41d-5c483277b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_fp_dataset.plot_spectrum(axis_name=\"time\")\n",
    "plt.yscale(\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fac525-4c15-4d0f-b35d-4604e87075c6",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "Estimate the goodness of this fit. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
